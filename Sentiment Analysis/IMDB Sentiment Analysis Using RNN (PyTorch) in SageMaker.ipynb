{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Using PyTorch in SageMaker\n",
    "\n",
    "_Deep Learning Nanodegree Program | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "**TODO** Intro\n",
    "\n",
    "> **NOTE**: In order to complete this notebook it is important that you give your notebook instance permission to access the Elastic Container Repository. This can be done by modifying the SageMaker Execution Role that was generated when the notebook instance was created. Make sure to the `AmazonEC2ContainerRegistryFullAccess` Policy.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this notebook. You will not need to modify the included code beyond what is requested. Sections that begin with '**TODO**' in the header indicate that you need to complete or implement some portion within them. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `# TODO: ...` comment. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions for you to answer which relate to the task and your implementation. Each section where you will answer a question is preceded by a '**Question:**' header. Carefully read each question and provide your answer below the '**Answer:**' header by editing the Markdown cell.\n",
    "\n",
    "> **Note**: Code and Markdown cells can be executed using the **Shift+Enter** keyboard shortcut. In addition, a cell can be edited by typically clicking it (double-click for Markdown cells) or by pressing **Enter** while it is highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data\n",
    "\n",
    "This notebook should be thought of as a continuation of the XGBoost in Sagemaker notebook. As such, we will be using some of the prepared data that was processed in the first notebook. If you have not yet run the first notebook, do so now so that the IMDB sentiment data will have been downloaded and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\") # where we will be reading the pre-computed data from\n",
    "\n",
    "def load_data(cache_dir = cache_dir, cache_file = \"preprocessed_data.pkl\"):\n",
    "    \n",
    "    # We will read in the cached data and then return the dataset\n",
    "    cache_data = None\n",
    "    with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "        cache_data = pickle.load(f)\n",
    "    print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "    \n",
    "    return cache_data['words_train'], cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Transform the data\n",
    "\n",
    "In the XGBoost notebook we transformed the data from its word representation to a bag-of-words feature representation. This time we would like to think of the various words as categorical variables, that is, we will represent each word as an integer. Of course, some of the words that appear in the reviews occur very infrequently and so likely don't contain much information for the purposes of sentiment analysis. The way we will deal with this problem is that we will fix the size of our working vocabularly and we will only include the words that appear most frequently. The will then combine all of the infrequent words into a single category and, in our case, we will label it as `1`.\n",
    "\n",
    "Furthermore, since we will be using a recurrent neural network, it will be convenient if the length of each review is the same. To do this, we will fix a size for our reviews and then pad short reviews with the category 'no word' (which we will label `0`) and truncate long reviews.\n",
    "\n",
    "** NOTE: ** As in the XGBoost notebook when we were creating the Bag-Of-Words features, we can only create our feature transformer using the training data, otherwise we are cheating by looking at the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a word dictionary\n",
    "\n",
    "To begin with, we need to construct a way to map words that appear in the reviews to integers. Here we fix the size of our vocabulary (including the 'no word' and 'infrequent' categories) to be `5000` but you may wish to change this to see how it affects the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    \n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word not in word_count: # We haven't come across this word yet\n",
    "                word_count[word] = 1\n",
    "            else:                      # Otherwise, increase the count\n",
    "                word_count[word] += 1\n",
    "                \n",
    "    # We only want to keep the most frequent words\n",
    "    sorted_words = [word for word in sorted(word_count, key=word_count.get, reverse=True)]\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' symbols\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook when we deploy our sentiment analysis model and make it accessible to the outside world, we will need to make use of this word dictionary. As such, we save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the reviews\n",
    "\n",
    "Now that we have our word dictionary which allows us to transform the words appearing in the reviews into integers, it is time to make use of it and convert our reviews to their integer representation, making sure to pad or truncate to a fixed length which in our case is `500`, but this could be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(data, word_dict, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the no word category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appear in the word_dict\n",
    "    \n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        working_sentence = [NOWORD] * pad\n",
    "        \n",
    "        # We go through each word in the (possibly truncated) sentence and convert the words to integers\n",
    "        for word_index, word in enumerate(sentence[:pad]):\n",
    "            if word in word_dict:\n",
    "                working_sentence[word_index] = word_dict[word]\n",
    "            else:\n",
    "                working_sentence[word_index] = INFREQ\n",
    "                \n",
    "        result.append(working_sentence)\n",
    "        lengths.append(min(len(sentence), pad)) # We will need to keep track of the length of each review for use later\n",
    "            \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad(train_X, word_dict)\n",
    "test_X, test_X_len = convert_and_pad(test_X, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the processed training dataset\n",
    "\n",
    "As in the XGBoost notebook, we will need to upload the training dataset to S3 in order for our training code to access it. For now we will save it locally and then we will upload to S3 later on.\n",
    "\n",
    "It is important to note the format of the data that we are saving as we will need to know it when we write the training code. In our case, each row of the dataset has the form `label`, `length`, `review` where `review` is a sequence of `500` integers representing the words in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datadir = './data/sentiment/pytorch'\n",
    "if not os.path.exists(datadir):\n",
    "    os.makedirs(datadir)\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(datadir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build and push the training container\n",
    "\n",
    "In the XGBoost notebook we discussed how what a model is in the SageMaker framework. In particular, a model comprises three objects\n",
    "\n",
    " - Model Artifacts,\n",
    " - Training Code (Container), and\n",
    " - Inference Code (Container),\n",
    " \n",
    "which interact with one another. In the XGBoost example we used training andi Inference code that was provided by Amazon, however, since we would now like to construct a custom model using PyTorch, we must write the code ourselves.\n",
    "\n",
    "Amazon SageMaker uses packages called 'containers' created using the Docker utility. Essentially, a Docker Container contains the complete specification of a computing environment along with the code which you want executed. In the example we are looking at now we have provided a Dockerfile which specifies the computing environment along with the training code.\n",
    "\n",
    "** Note: ** The compute environment we are using here requires a compute instance containing a GPU. We will discuss how to run this code on a compute instance which only has a CPU later in this notebook.\n",
    "\n",
    "In order to construct the required container we can use the provided `build_and_push.sh` shell script which will create the container and then upload it to Amazon's Elastic Container Repository. Once it has been uploaded (pushed) we can create a SageMaker estimator object which uses our custom code.\n",
    "\n",
    "To see the contents of the `build_and_push.sh` shell script, execute the cell below.\n",
    "\n",
    "** Note: ** The `build_and_push.sh` script uploads the container using the name `sentiment-pytorch-gpu`. This will be important later when we construct the SageMaker estimator and tell it to use our custom code. Also, if you wish to change the name of the container you are free to do so, however, note that the name of a container that is used in a SageMaker estimator object can only contain the characters a-z, A-Z, 0-9 and -."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\r\n",
      "\r\n",
      "if [ \"$1\" != \"gpu\" ] && [ \"$1\" != \"cpu\" ]; then\r\n",
      "    echo \"Must specify gpu or cpu\"\r\n",
      "    exit\r\n",
      "fi\r\n",
      "\r\n",
      "# The name of our algorithm\r\n",
      "algorithm_name=\"sentiment-pytorch-$1\"\r\n",
      "\r\n",
      "chmod +x sentiment/train\r\n",
      "chmod +x sentiment/serve\r\n",
      "\r\n",
      "account=$(aws sts get-caller-identity --query Account --output text)\r\n",
      "\r\n",
      "# Get the region defined in the current configuration (default to us-west-2 if none defined)\r\n",
      "region=$(aws configure get region)\r\n",
      "region=${region:-us-west-2}\r\n",
      "\r\n",
      "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\r\n",
      "\r\n",
      "# If the repository doesn't exist in ECR, create it.\r\n",
      "\r\n",
      "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\r\n",
      "\r\n",
      "if [ $? -ne 0 ]\r\n",
      "then\r\n",
      "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\r\n",
      "fi\r\n",
      "\r\n",
      "# Get the login command from ECR and execute it directly\r\n",
      "$(aws ecr get-login --region ${region} --no-include-email)\r\n",
      "\r\n",
      "# Build the docker image locally with the image name and then push it to ECR\r\n",
      "# with the full name.\r\n",
      "\r\n",
      "docker build  -t ${algorithm_name} -f Dockerfile.$1 .\r\n",
      "docker tag ${algorithm_name} ${fullname}\r\n",
      "\r\n",
      "docker push ${fullname}"
     ]
    }
   ],
   "source": [
    "!cat ./train_container/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the `build_and_push.sh` script creates two executable scripts, `train` and `serve`, when building the Docker container. The `serve` script will be discussed later on, for now we are interested in the `train` script. The `train` file is a Python script which is run when the container is executed in 'training mode', that is when it is being used to fit a model to some training data.\n",
    "\n",
    "The `train` script which has been provided is set up so that any modifications you would like to do to the RNN model for sentiment analysis can be done by modifying the model.py file instead.\n",
    "\n",
    "It is certainly worth taking a look at both the `train` script and the `model.py` script to see how they work. The actual details of the PyTorch implementation of a simple RNN model for sentiment analysis is not as important as understanding how the model is being used, trained, saved, etc... \n",
    "\n",
    "### Executing the shell script\n",
    "\n",
    "Once any changes have been made to the model it is time to actual build the container and upload it to the Elastic Container Repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/train_container\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  27.14kB\n",
      "Step 1/13 : FROM nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\n",
      " ---> 3c7ce06587fb\n",
      "Step 2/13 : RUN apt-get update && apt-get install -y     wget     curl     nginx     ca-certificates     sudo     git     bzip2     libx11-6  && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> bc2d7660c497\n",
      "Step 3/13 : RUN curl -so ~/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-4.4.10-Linux-x86_64.sh  && chmod +x ~/miniconda.sh  && ~/miniconda.sh -b -p /opt/conda  && rm ~/miniconda.sh\n",
      " ---> Using cache\n",
      " ---> 9fd66bf66fc4\n",
      "Step 4/13 : ENV PATH=/opt/conda/bin:$PATH\n",
      " ---> Using cache\n",
      " ---> 7f5f434553b5\n",
      "Step 5/13 : ENV CONDA_AUTO_UPDATE_CONDA=false\n",
      " ---> Using cache\n",
      " ---> fb2fcc7e0cb7\n",
      "Step 6/13 : RUN conda install -y \"conda>=4.4.11\" && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 814a6d32af35\n",
      "Step 7/13 : RUN conda install -y -c pytorch     cuda90     magma-cuda90     pytorch  && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 5be9e7b74a4e\n",
      "Step 8/13 : RUN conda install -y numpy pandas flask gevent gunicorn && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 8386e68d4caf\n",
      "Step 9/13 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 56aecbd63925\n",
      "Step 10/13 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 27e7907559ac\n",
      "Step 11/13 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> e250373640cd\n",
      "Step 12/13 : COPY sentiment /opt/program\n",
      " ---> Using cache\n",
      " ---> 9678edd9ef6a\n",
      "Step 13/13 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> c8aac2c578ba\n",
      "Successfully built c8aac2c578ba\n",
      "Successfully tagged sentiment-pytorch-gpu:latest\n",
      "The push refers to repository [337425718252.dkr.ecr.us-east-1.amazonaws.com/sentiment-pytorch-gpu]\n",
      "\n",
      "\u001b[1Bd5976f2b: Preparing \n",
      "\u001b[1Badee5418: Preparing \n",
      "\u001b[1Bd42ee2c3: Preparing \n",
      "\u001b[1B6e52a225: Preparing \n",
      "\u001b[1B82ac0279: Preparing \n",
      "\u001b[1B9a59a826: Preparing \n",
      "\u001b[1Bab55aa8c: Preparing \n",
      "\u001b[1Be046f000: Preparing \n",
      "\u001b[1B148cae00: Preparing \n",
      "\u001b[1Bcfebc71a: Preparing \n",
      "\u001b[1B07dad4ba: Preparing \n",
      "\u001b[1B9ebf93f3: Preparing \n",
      "\u001b[1B91e51d73: Preparing \n",
      "\u001b[1Bd9e65295: Preparing \n",
      "\u001b[1B45e78935: Preparing \n",
      "\u001b[1B1dc646ba: Preparing \n",
      "\u001b[1B79075e24: Layer already exists K\u001b[14A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[Klatest: digest: sha256:64fed9d7a0b73b5d8138e311062ebc05d401fe1feff9e4bc5619a47714966f22 size: 3895\n",
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "%cd train_container\n",
    "!chmod +x ./build_and_push.sh\n",
    "!./build_and_push.sh gpu\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and train the model\n",
    "\n",
    "Now that we have created the docker container we will be using to train our model, it is time to actually use it.\n",
    "\n",
    "### Uploading Training files to S3\n",
    "\n",
    "As in the XGBoost notebook, the training code that we have uploaded will have access to the training data that we choose by way of Amazon's S3 service. To give our training code access we will need to first upload the training data to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-337425718252\n"
     ]
    }
   ],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session() # Store the current SageMaker session\n",
    "\n",
    "#S3 prefix (which folder will we use)\n",
    "prefix = 'sentiment-pytorch'\n",
    "\n",
    "train_location = sess.upload_data(os.path.join(datadir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the RNN model\n",
    "\n",
    "Now that the data has been uploaded, it is time to construct the SageMaker estimator object. This will proceed much the same as the XGBoost example except that instead of using one of Amazon's containers we will be using the one that we constructed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To construct the model, remember that SageMaker need to know our current IAM role\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "\n",
    "# We need to get our working account number and region in order to fully specify the name of\n",
    "# the docker container we uploaded.\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "# This is the full name of our docker container. Remember that 'sentiment-pytorch-gpu' is the\n",
    "# name we used earlier when we created and pushed the container.\n",
    "training_image = '{}.dkr.ecr.{}.amazonaws.com/sentiment-pytorch-gpu'.format(account, region)\n",
    "\n",
    "# These are some additional hyperparameters which are passed to our custom code. To see how\n",
    "# this is used, see the code contained in model.py \n",
    "trainingParams = {\n",
    "    'batch_size': 512,\n",
    "}\n",
    "\n",
    "pytorch_model = sage.estimator.Estimator(training_image, role,  # We need to provided a link to our custom code\n",
    "                                        1, 'ml.p2.xlarge',      # This is the compute instance we are using, note that\n",
    "                                                                # the p2 instance are gpu instances\n",
    "                                        output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                                        hyperparameters=trainingParams, # Some model hyperparameters\n",
    "                                        sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sentiment-pytorch-gpu-2018-06-12-19-19-14-406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................\n",
      "\u001b[31mStarting the training.\u001b[0m\n",
      "\u001b[31mBuilding Model.\u001b[0m\n",
      "\u001b[31mLoading Data.\u001b[0m\n",
      "\u001b[31mTraining Model.\u001b[0m\n",
      "\u001b[31mEpoch:  1  BCELoss:  0.6856060198375157\u001b[0m\n",
      "\u001b[31mEpoch:  2  BCELoss:  0.6262111980087903\u001b[0m\n",
      "\u001b[31mEpoch:  3  BCELoss:  0.5698664954730442\u001b[0m\n",
      "\u001b[31mEpoch:  4  BCELoss:  0.5021822020715597\u001b[0m\n",
      "\u001b[31mEpoch:  5  BCELoss:  0.4423872183780281\u001b[0m\n",
      "\u001b[31mEpoch:  6  BCELoss:  0.41576231377465384\u001b[0m\n",
      "\u001b[31mEpoch:  7  BCELoss:  0.3875274706859978\u001b[0m\n",
      "\u001b[31mEpoch:  8  BCELoss:  0.3850110051583271\u001b[0m\n",
      "\u001b[31mEpoch:  9  BCELoss:  0.3551374187274855\u001b[0m\n",
      "\u001b[31mEpoch:  10  BCELoss:  0.34955783644501043\u001b[0m\n",
      "\u001b[31mSaving model.\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 289\n"
     ]
    }
   ],
   "source": [
    "pytorch_model.fit(train_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build and push the inference container\n",
    "\n",
    "As we discussed earlier in Step 3, a SageMaker model comprises three parts, the model artifacts, the training code and the inference code. So far we have used the first two objects, in particular we have built, pushed and used a training container and store the results which then become the model artifacts. However, what about the last part, the inference code?\n",
    "\n",
    "We also mentioned when we created the earlier docker container that there were two executed scripts included, `train` and `serve`, and that the `train` script was used for training. It should come as no surprise that the `serve` script is responsible for performing inference and so it is executed when the container is run in 'serving' or 'deployed' mode.\n",
    "\n",
    "So, if we wished, we could use the container we already created and, just like in the XGBoost notebook, we could deploy it and send it our test data. However, as noted earlier, the container that we created requires the compute instance to have a GPU. This seems excessive for our needs as performing prediction with an already fit model doesn't take nearly as many resources as actually training the model. Also, since in practice we don't know how long we will have the model deployed for we may wish to reduce costs and compute instances with GPUs are much more costly than CPU only compute instances.\n",
    "\n",
    "Fortunately, we do not need to change any of the code that we have written, we only need to change the environment in which the code is run. That is, we need to change the Dockerfile. A CPU based Dockerfile has been provided and we can construct the CPU version of our containiner using the same `build_and_push.sh` script as before. Note that in this case we call our container `pytorch-sentiment-cpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/train_container\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  27.14kB\n",
      "Step 1/13 : FROM ubuntu:16.04\n",
      " ---> 5e8b97a2a082\n",
      "Step 2/13 : RUN apt-get update && apt-get install -y     wget     curl     nginx     ca-certificates     sudo     git     bzip2     libx11-6  && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 2c5998119c7e\n",
      "Step 3/13 : RUN curl -so ~/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-4.4.10-Linux-x86_64.sh  && chmod +x ~/miniconda.sh  && ~/miniconda.sh -b -p /opt/conda  && rm ~/miniconda.sh\n",
      " ---> Using cache\n",
      " ---> 592b96ef1668\n",
      "Step 4/13 : ENV PATH=/opt/conda/bin:$PATH\n",
      " ---> Using cache\n",
      " ---> f37fc1f4c581\n",
      "Step 5/13 : ENV CONDA_AUTO_UPDATE_CONDA=false\n",
      " ---> Using cache\n",
      " ---> 3cbf326390ed\n",
      "Step 6/13 : RUN conda install -y \"conda>=4.4.11\" && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 59bf103b38fd\n",
      "Step 7/13 : RUN conda install -y -c pytorch     pytorch-cpu  && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 5bff1e05f514\n",
      "Step 8/13 : RUN conda install -y numpy pandas flask gevent gunicorn && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 8fd9d6f47742\n",
      "Step 9/13 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> fd71e7cb5b41\n",
      "Step 10/13 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 3b21aa83151f\n",
      "Step 11/13 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 5cac7ab746f1\n",
      "Step 12/13 : COPY sentiment /opt/program\n",
      " ---> Using cache\n",
      " ---> 7d128cc1f2de\n",
      "Step 13/13 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> b6d70ac0a260\n",
      "Successfully built b6d70ac0a260\n",
      "Successfully tagged sentiment-pytorch-cpu:latest\n",
      "The push refers to repository [337425718252.dkr.ecr.us-east-1.amazonaws.com/sentiment-pytorch-cpu]\n",
      "\n",
      "\u001b[1Bf0750d82: Preparing \n",
      "\u001b[1Bc9405d0b: Preparing \n",
      "\u001b[1Bc3cff060: Preparing \n",
      "\u001b[1B18bb392c: Preparing \n",
      "\u001b[1B7ad53523: Preparing \n",
      "\u001b[1B2fd6255b: Preparing \n",
      "\u001b[1B91e51d73: Preparing \n",
      "\u001b[1Bd9e65295: Preparing \n",
      "\u001b[1B45e78935: Preparing \n",
      "\u001b[1B1dc646ba: Preparing \n",
      "\u001b[5B91e51d73: Layer already exists \u001b[9A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[Klatest: digest: sha256:3c262b5a87704754794d85b828500f362d06e51b4897f86f646a4d7e3c8d6800 size: 2626\n",
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "%cd train_container\n",
    "!chmod +x ./build_and_push.sh\n",
    "!./build_and_push.sh cpu\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy and test the model\n",
    "\n",
    "Now that we have created a container for inference we can test the model that we created earlier. This essentially proceeds much the same as in the XGBoost example, the difference being that we explicitly tell SageMaker to use a different container when we deploy the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "Here we deploy the model we constructed earlier using the new CPU container we created. Note that if we leave out the optional parameter `image` to the deploy method then SageMaker defaults to using the container that was used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer # This helps specify how we want to send data to our inference code\n",
    "\n",
    "# As in the training step, this is the full name for the cpu container\n",
    "inference_image = '{}.dkr.ecr.{}.amazonaws.com/sentiment-pytorch-cpu:latest'.format(account, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sentiment-pytorch-cpu-2018-06-12-19-26-43-141\n",
      "INFO:sagemaker:Creating endpoint with name sentiment-pytorch-gpu-2018-06-12-19-19-14-406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "pytorch_predictor = pytorch_model.deploy(1, 'ml.m4.xlarge', # The type of compute instance used, note that m4 is a cpu instance\n",
    "                                        serializer=csv_serializer, # How do we want the data sent\n",
    "                                        image=inference_image) # Which container to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Now that the model has been deployed, it is running on an Amazon server somewhere. Now we will send it data and record the results so that we can see how well it performs on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=1000):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1)) # Break the data up into chunks\n",
    "    predictions = np.array([])\n",
    "    \n",
    "    for array in split_array:\n",
    "        chunk_predictions = pytorch_predictor.predict(array).decode('utf-8') # Get the predictions\n",
    "        chunk_predictions = np.fromstring(chunk_predictions, sep='\\n')       # Convert it to a numpy array\n",
    "        predictions = np.append(predictions, chunk_predictions)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that our custom inference code requires each input row to have the form `length`, `review` where `length` is the number of non-zero entries in `review` and `review` is a sequence of `500` integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comb = np.hstack((test_X_len.reshape([-1,1]), test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83316"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Now that we are done testing our model we need to delete the endpoint so that it is no longer running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sentiment-pytorch-gpu-2018-06-12-19-19-14-406\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(pytorch_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
