{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Building an API in SageMaker\n",
    "\n",
    "_Deep Learning Nanodegree Program | Deployment_\n",
    "\n",
    "---\n",
    "\n",
    "**TODO** Intro\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this notebook. You will not need to modify the included code beyond what is requested. Sections that begin with '**TODO**' in the header indicate that you need to complete or implement some portion within them. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `# TODO: ...` comment. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions for you to answer which relate to the task and your implementation. Each section where you will answer a question is preceded by a '**Question:**' header. Carefully read each question and provide your answer below the '**Answer:**' header by editing the Markdown cell.\n",
    "\n",
    "> **Note**: Code and Markdown cells can be executed using the **Shift+Enter** keyboard shortcut. In addition, a cell can be edited by typically clicking it (double-click for Markdown cells) or by pressing **Enter** while it is highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the inference code.\n",
    "\n",
    "In the previous notebook we constructed a custom model and two different docker containers to manipulate it. The first container we used for training and the second we used for inference. However, we made the assumption when constructing the inference container that the input would be a review described as a seqeunce of integers. However, our goal is to create a simple web app that allows a user to type out a review and then tells the user whether their review is positive or negative. This means we need to modify our inference code to accept a string and then transform the input inside the inference container.\n",
    "\n",
    "To begin with, let us remind ourselves how we process a review in order to send it off for inference. To begin with, we will read one of the reviews in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "review_text = None\n",
    "with open(os.path.join('aclImdb', 'test', 'pos', '10000_7.txt')) as f:\n",
    "    review_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Actor turned director Bill Paxton follows up his promising debut, the Gothic-horror \"Frailty\", with this family friendly sports drama about the 1913 U.S. Open where a young American caddy rises from his humble background to play against his Bristish idol in what was dubbed as \"The Greatest Game Ever Played.\" I\\'m no fan of golf, and these scrappy underdog sports flicks are a dime a dozen (most recently done to grand effect with \"Miracle\" and \"Cinderella Man\"), but some how this film was enthralling all the same.<br /><br />The film starts with some creative opening credits (imagine a Disneyfied version of the animated opening credits of HBO\\'s \"Carnivale\" and \"Rome\"), but lumbers along slowly for its first by-the-numbers hour. Once the action moves to the U.S. Open things pick up very well. Paxton does a nice job and shows a knack for effective directorial flourishes (I loved the rain-soaked montage of the action on day two of the open) that propel the plot further or add some unexpected psychological depth to the proceedings. There\\'s some compelling character development when the British Harry Vardon is haunted by images of the aristocrats in black suits and top hats who destroyed his family cottage as a child to make way for a golf course. He also does a good job of visually depicting what goes on in the players\\' heads under pressure. Golf, a painfully boring sport, is brought vividly alive here. Credit should also be given the set designers and costume department for creating an engaging period-piece atmosphere of London and Boston at the beginning of the twentieth century.<br /><br />You know how this is going to end not only because it\\'s based on a true story but also because films in this genre follow the same template over and over, but Paxton puts on a better than average show and perhaps indicates more talent behind the camera than he ever had in front of it. Despite the formulaic nature, this is a nice and easy film to root for that deserves to find an audience.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've read a sample review, the first thing we need to do is remove the html tags and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actor',\n",
       " 'turn',\n",
       " 'director',\n",
       " 'bill',\n",
       " 'paxton',\n",
       " 'follow',\n",
       " 'promis',\n",
       " 'debut',\n",
       " 'gothic',\n",
       " 'horror',\n",
       " 'frailti',\n",
       " 'famili',\n",
       " 'friendli',\n",
       " 'sport',\n",
       " 'drama',\n",
       " '1913',\n",
       " 'u',\n",
       " 'open',\n",
       " 'young',\n",
       " 'american',\n",
       " 'caddi',\n",
       " 'rise',\n",
       " 'humbl',\n",
       " 'background',\n",
       " 'play',\n",
       " 'bristish',\n",
       " 'idol',\n",
       " 'dub',\n",
       " 'greatest',\n",
       " 'game',\n",
       " 'ever',\n",
       " 'play',\n",
       " 'fan',\n",
       " 'golf',\n",
       " 'scrappi',\n",
       " 'underdog',\n",
       " 'sport',\n",
       " 'flick',\n",
       " 'dime',\n",
       " 'dozen',\n",
       " 'recent',\n",
       " 'done',\n",
       " 'grand',\n",
       " 'effect',\n",
       " 'miracl',\n",
       " 'cinderella',\n",
       " 'man',\n",
       " 'film',\n",
       " 'enthral',\n",
       " 'film',\n",
       " 'start',\n",
       " 'creativ',\n",
       " 'open',\n",
       " 'credit',\n",
       " 'imagin',\n",
       " 'disneyfi',\n",
       " 'version',\n",
       " 'anim',\n",
       " 'open',\n",
       " 'credit',\n",
       " 'hbo',\n",
       " 'carnival',\n",
       " 'rome',\n",
       " 'lumber',\n",
       " 'along',\n",
       " 'slowli',\n",
       " 'first',\n",
       " 'number',\n",
       " 'hour',\n",
       " 'action',\n",
       " 'move',\n",
       " 'u',\n",
       " 'open',\n",
       " 'thing',\n",
       " 'pick',\n",
       " 'well',\n",
       " 'paxton',\n",
       " 'nice',\n",
       " 'job',\n",
       " 'show',\n",
       " 'knack',\n",
       " 'effect',\n",
       " 'directori',\n",
       " 'flourish',\n",
       " 'love',\n",
       " 'rain',\n",
       " 'soak',\n",
       " 'montag',\n",
       " 'action',\n",
       " 'day',\n",
       " 'two',\n",
       " 'open',\n",
       " 'propel',\n",
       " 'plot',\n",
       " 'add',\n",
       " 'unexpect',\n",
       " 'psycholog',\n",
       " 'depth',\n",
       " 'proceed',\n",
       " 'compel',\n",
       " 'charact',\n",
       " 'develop',\n",
       " 'british',\n",
       " 'harri',\n",
       " 'vardon',\n",
       " 'haunt',\n",
       " 'imag',\n",
       " 'aristocrat',\n",
       " 'black',\n",
       " 'suit',\n",
       " 'top',\n",
       " 'hat',\n",
       " 'destroy',\n",
       " 'famili',\n",
       " 'cottag',\n",
       " 'child',\n",
       " 'make',\n",
       " 'way',\n",
       " 'golf',\n",
       " 'cours',\n",
       " 'also',\n",
       " 'good',\n",
       " 'job',\n",
       " 'visual',\n",
       " 'depict',\n",
       " 'goe',\n",
       " 'player',\n",
       " 'head',\n",
       " 'pressur',\n",
       " 'golf',\n",
       " 'pain',\n",
       " 'bore',\n",
       " 'sport',\n",
       " 'brought',\n",
       " 'vividli',\n",
       " 'aliv',\n",
       " 'credit',\n",
       " 'also',\n",
       " 'given',\n",
       " 'set',\n",
       " 'design',\n",
       " 'costum',\n",
       " 'depart',\n",
       " 'creat',\n",
       " 'engag',\n",
       " 'period',\n",
       " 'piec',\n",
       " 'atmospher',\n",
       " 'london',\n",
       " 'boston',\n",
       " 'begin',\n",
       " 'twentieth',\n",
       " 'centuri',\n",
       " 'know',\n",
       " 'go',\n",
       " 'end',\n",
       " 'base',\n",
       " 'true',\n",
       " 'stori',\n",
       " 'also',\n",
       " 'film',\n",
       " 'genr',\n",
       " 'follow',\n",
       " 'templat',\n",
       " 'paxton',\n",
       " 'put',\n",
       " 'better',\n",
       " 'averag',\n",
       " 'show',\n",
       " 'perhap',\n",
       " 'indic',\n",
       " 'talent',\n",
       " 'behind',\n",
       " 'camera',\n",
       " 'ever',\n",
       " 'front',\n",
       " 'despit',\n",
       " 'formula',\n",
       " 'natur',\n",
       " 'nice',\n",
       " 'easi',\n",
       " 'film',\n",
       " 'root',\n",
       " 'deserv',\n",
       " 'find',\n",
       " 'audienc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_words = review_to_words(review_text)\n",
    "review_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, now that we've converted our review into usable words we need to map those words to integers using the `word_dict` that we created using the training set. We also need to pad or truncate the resulting sequence if it isn't the correct size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "word_dict = None\n",
    "with open(\"word_dict.pkl\", \"rb\") as f:\n",
    "    word_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(data, word_dict, pad=500):\n",
    "    NOWORD = 0 # Use 0 to represent the no word category\n",
    "    INFREQ = 1 # Use 1 to represent infrequent words\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    # We go through each word in the (possibly truncated) review and convert the words to integers\n",
    "    for word_index, word in enumerate(data[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(data), pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data, review_length = convert_and_pad(review_words, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186,\n",
       " [68,\n",
       "  99,\n",
       "  100,\n",
       "  819,\n",
       "  1,\n",
       "  237,\n",
       "  1124,\n",
       "  2147,\n",
       "  3435,\n",
       "  132,\n",
       "  1,\n",
       "  180,\n",
       "  2914,\n",
       "  1605,\n",
       "  568,\n",
       "  1,\n",
       "  3471,\n",
       "  248,\n",
       "  107,\n",
       "  212,\n",
       "  1,\n",
       "  1387,\n",
       "  3998,\n",
       "  984,\n",
       "  32,\n",
       "  1,\n",
       "  1,\n",
       "  1352,\n",
       "  704,\n",
       "  465,\n",
       "  64,\n",
       "  32,\n",
       "  175,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1605,\n",
       "  591,\n",
       "  1,\n",
       "  1696,\n",
       "  524,\n",
       "  185,\n",
       "  1698,\n",
       "  176,\n",
       "  4801,\n",
       "  2843,\n",
       "  88,\n",
       "  5,\n",
       "  1,\n",
       "  5,\n",
       "  94,\n",
       "  1276,\n",
       "  248,\n",
       "  532,\n",
       "  449,\n",
       "  1,\n",
       "  242,\n",
       "  247,\n",
       "  248,\n",
       "  532,\n",
       "  4859,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  318,\n",
       "  1436,\n",
       "  27,\n",
       "  424,\n",
       "  294,\n",
       "  155,\n",
       "  223,\n",
       "  3471,\n",
       "  248,\n",
       "  38,\n",
       "  500,\n",
       "  51,\n",
       "  1,\n",
       "  227,\n",
       "  264,\n",
       "  29,\n",
       "  1,\n",
       "  176,\n",
       "  3200,\n",
       "  1,\n",
       "  33,\n",
       "  2344,\n",
       "  1,\n",
       "  3460,\n",
       "  155,\n",
       "  194,\n",
       "  48,\n",
       "  248,\n",
       "  1,\n",
       "  65,\n",
       "  480,\n",
       "  2096,\n",
       "  1459,\n",
       "  1178,\n",
       "  3293,\n",
       "  1212,\n",
       "  18,\n",
       "  426,\n",
       "  603,\n",
       "  990,\n",
       "  1,\n",
       "  1142,\n",
       "  754,\n",
       "  1,\n",
       "  271,\n",
       "  1181,\n",
       "  362,\n",
       "  2547,\n",
       "  993,\n",
       "  180,\n",
       "  1,\n",
       "  612,\n",
       "  8,\n",
       "  46,\n",
       "  1,\n",
       "  383,\n",
       "  26,\n",
       "  14,\n",
       "  264,\n",
       "  571,\n",
       "  816,\n",
       "  193,\n",
       "  1235,\n",
       "  320,\n",
       "  3755,\n",
       "  1,\n",
       "  576,\n",
       "  300,\n",
       "  1605,\n",
       "  692,\n",
       "  1,\n",
       "  1890,\n",
       "  532,\n",
       "  26,\n",
       "  266,\n",
       "  110,\n",
       "  909,\n",
       "  1056,\n",
       "  2672,\n",
       "  311,\n",
       "  1116,\n",
       "  939,\n",
       "  285,\n",
       "  815,\n",
       "  1629,\n",
       "  1,\n",
       "  188,\n",
       "  1,\n",
       "  1425,\n",
       "  39,\n",
       "  24,\n",
       "  40,\n",
       "  335,\n",
       "  253,\n",
       "  20,\n",
       "  26,\n",
       "  5,\n",
       "  721,\n",
       "  237,\n",
       "  1,\n",
       "  1,\n",
       "  129,\n",
       "  74,\n",
       "  889,\n",
       "  29,\n",
       "  338,\n",
       "  2086,\n",
       "  377,\n",
       "  431,\n",
       "  354,\n",
       "  64,\n",
       "  940,\n",
       "  404,\n",
       "  1850,\n",
       "  464,\n",
       "  227,\n",
       "  758,\n",
       "  5,\n",
       "  1819,\n",
       "  470,\n",
       "  61,\n",
       "  244,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_length, review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have input that can be sent to the neural network that we trained previously. To reiterate, given a review in string form, we need to do the following in order to determine the sentiment of the review:\n",
    "\n",
    "- Convert the review to words (clean html, remove stopwords, etc.)\n",
    "- Transform the words to integers using `word_dict` and pad / truncate the sequence\n",
    "- Send the data through the neural network.\n",
    "\n",
    "The important takeaway here isn't the additional pre-processing of the input, this is relatively easy to do you just need to modify the code in either `train` or in `model.py` to incorporate this step. Instead, it is important to note that we need to include the `word_dict.pkl` file so that our inference code can make use of it to perform the second item above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build and Push new inference code\n",
    "\n",
    "Now that we know what our inference code needs to do, we can make the necessary changes. The code for this has been provided and resides in the `api_container` folder. In particular, note that the `sentiment_api.py` file contains the code shown above to pre-process incoming data. Of course, in order to do this we need to make sure to include `word_dict.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp word_dict.pkl api_container/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, the changes between the original inference code and our new inference code are the following:\n",
    "\n",
    "- `predictor.py` has been modified to pre-process incoming data,\n",
    "- `sentiment_api.py` has been added, implementing the pre-processing methods,\n",
    "- `word_dict.pkl` has been added,\n",
    "- `train` has been removed so that this container can't accidentally be used for training, and\n",
    "- `Dockerfile.cpu` has been modified so that our code has access to the nltk and BeautifulSoup libraries.\n",
    "\n",
    "Now that this is done, we can run the `build_and_push.sh` script to make our container available on Amazon's Elastic Container Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/api_container\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  121.3kB\n",
      "Step 1/13 : FROM ubuntu:16.04\n",
      " ---> 5e8b97a2a082\n",
      "Step 2/13 : RUN apt-get update && apt-get install -y     wget     curl     nginx     ca-certificates     sudo     git     bzip2     libx11-6  && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> c3ecf8be4702\n",
      "Step 3/13 : RUN curl -so ~/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-4.4.10-Linux-x86_64.sh  && chmod +x ~/miniconda.sh  && ~/miniconda.sh -b -p /opt/conda  && rm ~/miniconda.sh\n",
      " ---> Using cache\n",
      " ---> f37f52cc54c8\n",
      "Step 4/13 : ENV PATH=/opt/conda/bin:$PATH\n",
      " ---> Using cache\n",
      " ---> e33b6737e0eb\n",
      "Step 5/13 : ENV CONDA_AUTO_UPDATE_CONDA=false\n",
      " ---> Using cache\n",
      " ---> 990689859995\n",
      "Step 6/13 : RUN conda install -y \"conda>=4.4.11\" && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 46a9cdb90f38\n",
      "Step 7/13 : RUN conda install -y -c pytorch     pytorch-cpu  && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 77f1d1aed2ef\n",
      "Step 8/13 : RUN conda install -y numpy pandas flask gevent gunicorn nltk beautifulsoup4 html5lib && conda clean -ya\n",
      " ---> Using cache\n",
      " ---> 3138615e7a3f\n",
      "Step 9/13 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 1421a361ee69\n",
      "Step 10/13 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 75c036cffa8c\n",
      "Step 11/13 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> cf1bc7419953\n",
      "Step 12/13 : COPY sentiment /opt/program\n",
      " ---> Using cache\n",
      " ---> 240ddfed7b64\n",
      "Step 13/13 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> 5418e6190cdb\n",
      "Successfully built 5418e6190cdb\n",
      "Successfully tagged sentiment-pytorch-api:latest\n",
      "The push refers to repository [337425718252.dkr.ecr.us-east-1.amazonaws.com/sentiment-pytorch-api]\n",
      "\n",
      "\u001b[1B8fc9b54c: Preparing \n",
      "\u001b[1B08cc017a: Preparing \n",
      "\u001b[1Bf410fb59: Preparing \n",
      "\u001b[1Bf7f11d4e: Preparing \n",
      "\u001b[1Becb6c93a: Preparing \n",
      "\u001b[1B57aebdb6: Preparing \n",
      "\u001b[1B91e51d73: Preparing \n",
      "\u001b[1Bd9e65295: Preparing \n",
      "\u001b[1B45e78935: Preparing \n",
      "\u001b[1B1dc646ba: Preparing \n",
      "\u001b[1B79075e24: Layer already exists K\u001b[7A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[Klatest: digest: sha256:ff7c572161de93b54487a41f528c3ba4bcb75d3f0f8dd6c7c68012f29cbb2671 size: 2627\n",
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "%cd api_container\n",
    "!chmod +x ./build_and_push.sh\n",
    "!./build_and_push.sh\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test the new inference container\n",
    "\n",
    "Before getting into the details of setting up the web app we should make sure that our new inference container behaves the way we expect it to. To do this we will deploy and test our new container. Now, the way in which we will do this is a little different from the way that we did it in the previous notebook. This is because we want to use the model artifacts that we created in the previous notebook rather than creating a new model and training it from scratch.\n",
    "\n",
    "### Creating the endpoint\n",
    "\n",
    "Of course, we need to know where those model artifacts are stored. Luckily, we can look this up using the SageMaker console. First, click on **Models** to see the models that you have created. Then, select the model you would like to use. In our case it will likely be the most recently created model. Lastly, look at the details of the model settings and copy and paste the link displayed under **Location of model artifacts**. This link should begin with `s3://` and end with `model.tar.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifacts = \"s3://sagemaker-us-east-1-337425718252/output/sentiment-pytorch-gpu-2018-06-13-10-49-41-996/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know where the model artifacts are stored, we can construct an endpoint using these model artifacts along with the inference container that we've built. To do this we will use the `endpoint_from_model_data()` method provided by the SageMaker Session object. For more details and additional methods provided by the Session object please consult the [SageMaker documentation](http://sagemaker.readthedocs.io)\n",
    "\n",
    "**Note**: It is important to name the endpoint something that you will remember as it will be required in the Lambda function that we create later to access the inference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session() # Store the current SageMaker session\n",
    "role = sage.get_execution_role() # Store our current IAM role\n",
    "\n",
    "# We will also need our current account number and region in order to completely specify\n",
    "# the name of the docker container we created earlier\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "inference_image = '{}.dkr.ecr.{}.amazonaws.com/sentiment-pytorch-api'.format(account, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint with name SentimentAnalysisEndpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "model_endpoint = sess.endpoint_from_model_data(model_artifacts, # Where the model artifacts are stored\n",
    "                                              inference_image,  # Which container to use for inference\n",
    "                                              1, 'ml.m4.xlarge',# What sort of compute instance to use\n",
    "                                              name=\"SentimentAnalysisEndpoint\", # The name of the endpoint\n",
    "                                              role = role)      # Our current role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our inference code\n",
    "\n",
    "Now that we have constructed the endpoint it is time to use it. To do so we will first create a predictor object and then send some data to it. In order to construct the predictor we need to know the name of the endpoint that we've just created. Fortunately, this is returned by the `endpoint_from_model_data()` method used earlier. We also need to tell SageMaker the format that we expect to use in order to send data. Since we want to send a string (the review itself) we set the content type to `text/plain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sage.predictor.RealTimePredictor(model_endpoint, content_type='text/plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, we send some reviews to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='aclImdb', stop=250):\n",
    "    \n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # We make sure to test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                results.append(float(predictor.predict(review_input)))\n",
    "                \n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\n",
    "            # only send a small number of reviews\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting  pos  files\n",
      "Starting  neg  files\n"
     ]
    }
   ],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Exposing our endpoint to the outside world\n",
    "\n",
    "Currently we have been access the model endpoint by constructing a predictor object which uses the endpoint and then just using the predictor object to perform inference. What if we wanted to create a web app which accessed our model? The way things are set up currently makes that not possible since in order to access a SageMaker endpoint the app would first have to authenticate with AWS using an IAM role which included access to SageMaker endpoints. However, there is an easier way! We just need to use some additional AWS services.\n",
    "\n",
    "There are two services that we will be using to allow access to our model from the outside world. The first is called Lambda and the second is API Gatway.\n",
    "\n",
    "Lambda is a service which allows someone to write some relatively simple code and have it executed whenever a chosen trigger occurs. For example, you may want to update a database whenever new data is uploaded to a folder stored on S3.\n",
    "\n",
    "API Gateway is a service that allows you to create HTTP endpoints (url addresses) which are connected to other AWS services. One of the benefits to this is that you get to decide what credentials, if any, are required to access these endpoints.\n",
    "\n",
    "In our case we are going to set up an HTTP endpoint through API Gateway which is open to the public. Then, whenever anyone sends data to our public endpoint we will have that trigger a Lambda function which will send the input (in our case a review) to the inference container and return the result.\n",
    "\n",
    "> TODO: Include an image to help describe this.\n",
    "\n",
    "### Setting up a Lambda function\n",
    "\n",
    "The first thing we are going to do is set up a Lambda function. This Lambda function will be executed whenever our public API has data sent to it. When it is executed it will receive the data, perform any sort of processing that is required, send the data (the review) to the SageMaker endpoint we've created and then return the result.\n",
    "\n",
    "#### Part A: Create an IAM Role for the Lambda function\n",
    "\n",
    "Since we want the Lambda function to call a SageMaker endpoint, we need to make sure that it has permission to do so. To do this, we will construct a role that we can later give the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the **IAM** page and click on **Roles**. Then, click on **Create role**. Make sure that the **AWS service** is the type of trusted entity selected and choose **Lambda** as the service that will use this role, then click **Next: Permissions**.\n",
    "\n",
    "In the search box type `sagemaker` and select the check box next to the **AmazonSageMakerFullAccess** policy. Then, click on **Next: Review**.\n",
    "\n",
    "Lastly, give this role a name. Make sure you use a name that you will remember later on, for example `LambdaSageMakerRole`. Then, click on **Create role**.\n",
    "\n",
    "#### Part B: Create a Lambda function\n",
    "\n",
    "Now it is time to actually create the Lambda function.\n",
    "\n",
    "Using the AWS Console, navigate to the AWS Lambda page and click on **Create a function**. When you get to the next page, make sure that **Author from scratch** is selected. Now, name your Lambda function, using a name that you will remember later on, for example `sentiment_analysis_func`. Make sure that the **Python 3.6** runtime is selected and then choose the role that you created in the previous part. Then, click on **Create Function**.\n",
    "\n",
    "On the next page you will see some information about the Lambda function you've just created. If you scroll down you should see an editor in which you can write the code that will be executed when your Lambda function is triggered. In our example, we will use the code below:\n",
    "\n",
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "# The data will be sent to us, ultimately, from a web page which will encode the input as\n",
    "# html form data. This allows us to parse information in that format.\n",
    "from urllib.parse import parse_qs\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # The 'body' element in the 'event' object contains the content of the information sent\n",
    "    # to us. This will be url encoded data so we use parse_qs to parse it. We can then extract\n",
    "    # the review that was sent.\n",
    "    review = parse_qs(event['body'])['review'][0]\n",
    "    \n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "    \n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = 'SentimentAnalysisEndpoint', # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = review)                              # The actual review\n",
    "    \n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    if float(result) == 0:\n",
    "        result = \"Negative\"\n",
    "    else:\n",
    "        result = \"Positive\"\n",
    "    \n",
    "    # Return a proper response object as we will be using proxy integration.\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/html' },\n",
    "        'body' : \"<html> \\\n",
    "                    <body> \\\n",
    "                      <p>The review was {}</p> \\\n",
    "                      <p>Review:<br>{}</p> \\\n",
    "                    </body> \\\n",
    "                  </html>\".format(result, review)\n",
    "    }\n",
    "```\n",
    "\n",
    "Once you have copy and pasted the code above into the code editor, click on **Save** and your Lambda function will be up and running. Now we need to create a way for our web app to execute the Lambda function.\n",
    "\n",
    "### Setting up API Gateway\n",
    "\n",
    "Now that our Lambda function is set up, it is time to create a new API using API Gateway that will trigger the Lambda function we have just created.\n",
    "\n",
    "Using AWS Console, navigate to **Amazon API Gateway** and then click on **Get started**.\n",
    "\n",
    "On the next page, make sure that **New API** is selected and give the new api a name, for example, `sentiment_analysis_api`. Then, click on **Create API**.\n",
    "\n",
    "Now we have created an API, however it doesn't currently do anything. What we want it to do is to trigger the Lambda function that we created earlier.\n",
    "\n",
    "Select the **Actions** dropdown menu and click **Create Method**. A new blank method will be created, select its dropdown menu and select **POST**, then click on the check mark beside it.\n",
    "\n",
    "For the integration point, make sure that **Lambda Function** is selected and click on the **Use Lambda Proxy integration**. This option makes sure that the data that is sent to the API is then sent directly to the Lambda function with no processing. It also means that the return value must be a proper response object as it will also not be processed by API Gateway.\n",
    "\n",
    "Type the name of the Lambda function you created earlier into the **Lambda Function** text entry box and then click on **Save**. Click on **OK** in the pop-up box that then appears, giving permission to API Gateway to invoke the Lambda function you created.\n",
    "\n",
    "The last step in creating the API Gateway is to select the **Actions** dropdown and click on **Deploy API**. You will need to create a new Deployment stage and name it anything you like, for example `prod`.\n",
    "\n",
    "You have now successfully set up a public API to access your SageMaker model. Make sure to copy or write down the URL provided to invoke your newly created public API as this will be needed in the next step. This URL can be found at the top of the page, highlighted in blue next to the text **Invoke URL**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deploying our web app\n",
    "\n",
    "Now that we have a publicly available API, we can start using it in a web app. For our purposes, we have provided a simple static html file which can make use of the public api you created earlier.\n",
    "\n",
    "In the `website` folder there should be a file called `index.html`. Download the file to your computer and open that file up in a text editor of your choice. There should be a line which contains **\\*\\*REPLACE WITH PUBLIC API URL\\*\\***. Replace this string with the url that you wrote down in the last step and then save the file.\n",
    "\n",
    "Now, if you open `index.html` on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.\n",
    "\n",
    "If you'd like to go further, you can host this html file anywhere you'd like, for example using github or hosting a static site on Amazon's S3. Once you have done this you can share the link with anyone you'd like and have them play with it too!\n",
    "\n",
    "> **Important Note** In order for the web app to communicate with the SageMaker endpoint, the endpoint has to actually be deployed and running. This means that you are paying for it. Make sure that the endpoint is running when you want to use the web app but that you shut it down when you don't need it, otherwise you will end up with a surprisingly large AWS bill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Now that we are done testing our model we need to delete the endpoint so that it is no longer running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: SentimentAnalysisEndpoint\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(model_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
